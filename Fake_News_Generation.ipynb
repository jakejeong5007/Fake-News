{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake News Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "031d2efbc1de4eb89d5de65084c2171a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e6f77cb1f93f48719bd2a9cd298237d8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_48b4b95e39e34f32998c20426ba6d273",
              "IPY_MODEL_27cf0b6158f14e71b12a61b70e0f5579"
            ]
          }
        },
        "e6f77cb1f93f48719bd2a9cd298237d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "48b4b95e39e34f32998c20426ba6d273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "TextView",
            "style": "IPY_MODEL_2daf6050a6ea4cbf84b4335164d4f98b",
            "_dom_classes": [],
            "description": "text",
            "_model_name": "TextModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "abc",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "continuous_update": true,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7faf248be845480f8b709742ee9ba7f7"
          }
        },
        "27cf0b6158f14e71b12a61b70e0f5579": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_view_name": "OutputView",
            "msg_id": "",
            "_dom_classes": [],
            "_model_name": "OutputModel",
            "outputs": [
              {
                "output_type": "display_data",
                "metadata": {
                  "tags": []
                },
                "text/plain": "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0]])"
              }
            ],
            "_view_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_view_count": null,
            "_view_module_version": "1.0.0",
            "layout": "IPY_MODEL_9e95e2005a494904a881daee8d5197c4",
            "_model_module": "@jupyter-widgets/output"
          }
        },
        "2daf6050a6ea4cbf84b4335164d4f98b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7faf248be845480f8b709742ee9ba7f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e95e2005a494904a881daee8d5197c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cedd940207594f87a7a824dad29a6e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_363f72ef37324d5ca25de597b7193ba9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7da9956d56704b358615476e45eaeb57",
              "IPY_MODEL_69daf04ddd494d499e2ae82796986ff4"
            ]
          }
        },
        "363f72ef37324d5ca25de597b7193ba9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7da9956d56704b358615476e45eaeb57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "TextView",
            "style": "IPY_MODEL_c5751345899e4602aed08204b2d39238",
            "_dom_classes": [],
            "description": "sequence",
            "_model_name": "TextModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "continuous_update": true,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_092b7f1a741147f0829608878e944539"
          }
        },
        "69daf04ddd494d499e2ae82796986ff4": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_view_name": "OutputView",
            "msg_id": "",
            "_dom_classes": [],
            "_model_name": "OutputModel",
            "outputs": [
              {
                "output_type": "stream",
                "metadata": {
                  "tags": []
                },
                "text": "waiting...\n",
                "stream": "stdout"
              }
            ],
            "_view_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_view_count": null,
            "_view_module_version": "1.0.0",
            "layout": "IPY_MODEL_056760ac32ba47ffa22a7648c5b35dac",
            "_model_module": "@jupyter-widgets/output"
          }
        },
        "c5751345899e4602aed08204b2d39238": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "092b7f1a741147f0829608878e944539": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "056760ac32ba47ffa22a7648c5b35dac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ec31b1b01cf24a2192649ef927ccbcf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9da861a2b4de490d9b629b394ede3eba",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b59820fe536b43329a907b2a372efd33",
              "IPY_MODEL_1e182c98fd1b4cf5a9a7c6dc43582f80"
            ]
          }
        },
        "9da861a2b4de490d9b629b394ede3eba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b59820fe536b43329a907b2a372efd33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "TextView",
            "style": "IPY_MODEL_331c8cb47e7e411290a7acb6960e04f8",
            "_dom_classes": [],
            "description": "sequence",
            "_model_name": "TextModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Ing",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "continuous_update": true,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b01c4dd80d4b4376ac904349ca1f79de"
          }
        },
        "1e182c98fd1b4cf5a9a7c6dc43582f80": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_view_name": "OutputView",
            "msg_id": "",
            "_dom_classes": [],
            "_model_name": "OutputModel",
            "outputs": [
              {
                "output_type": "display_data",
                "metadata": {
                  "tags": [],
                  "needs_background": "light"
                },
                "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAXjElEQVR4nO3de7QlZX3m8e9Dc5FbMEprpLuhERBtL1FsEZcanQSXIKOYRJcwOgGFEFG84Y0kxjioEcWos5YkiuJSRAVivHQCDo4gEDUIjeIFEG0RpBvURkVFJgL6mz+qDpaHc9nd7HNO99vfz1q92FX1Vr3vrnrr2VVv7X1IVSFJ2vxttdANkCSNh4EuSY0w0CWpEQa6JDXCQJekRhjoktSILS7Qk3wwyZv6109Mcs081VtJ9p6Hepb3dW29ketP284kz03y2anKJnlPkr+bYbt/k+T9G9OmjZXk2CQ/THJrkvuOUP7IJF+Yh3bNS19YCEn2TXJFkl8keeksZX9nf497v8xXn0vymSRHzHU9o9iok36uJbkOuD/wa+CXwGeA46rq1nHWU1X/Aew7QnuOBI6uqieMs/7NTVV9BPjINMteOPE6yZOBM6pq6WD5P8x5AweSbAO8Azigqr42xfLlwPeAbarqzvls26aiP8+OrqrPjXGzrwE+X1WPHOM2N8pc9rkkVVXp6zl4TNtcDlxYVcs3dhub8hX606tqJ2A/YCXwuskFNvYqdHO2Jb7njXR/4F7AlQvdkLmykH0hnanyYw8a3uebuk050AGoqnV0V+gPg7tuy16c5DvAd/p5/72/zbslyZeSPGJi/SSPSvKV/hbwLLqTfGLZk5OsHUwvS/KJJOuT/DjJu5M8BHgP8Lj+1v2Wvux2Sd6e5Pv9bf17kmw/2Nark9yU5MYkL5jpPSa5MMlbklya5OdJPp3kPv2yiSGUo5J8H7ggyVZJXpfk+iQ/SnJ6kl0mbfYFfd03JXnVoK79k/xnv69u6t/jtpPWfVqSa5PcnOTkiRN3piGJiaGsJDv2x2u3fn/dmmS3JG9Icsag/AH9sbolydf6q/qJZUf29f8iyfeSPHeaOrdL8q7+fd7Yv94uyYOAiaG0W5JcMMXqFw+W35rkcYPtvj3JT/u6Dx7M3yXJaf1+W9e/30XTtG1Rf8v/3f59XJ5k2aDIgUm+07//U5KkX2+vJBf0/e/mJB9Jcu/Bdq9L8tokXwd+mWTrJCcM6rkqyZ9OastfJrl6sHy/JB8Gdgf+rX//rxnhuFyY5M1JvgjcBjxwUj0XAP8NeHe/zQf1++z0dOfU9X2/nTV3Zlqvn350//q5/fnx0H76qCSf6l/f1efy2/PoiHTn7M1J/nZQ3/ZJPtQf96uTvCaDbJilrRcmObp/fWSSL8zQh/ZMcnF/LD7XH/szpt/6BqqqTe4fcB1wYP96Gd0n/hv76QL+L3AfYHvgUcCPgMcCi4Aj+vW3A7YFrgdeAWwDPAu4A3hTv60nA2v714uArwHvBHakC/4n9MuOBL4wqY3vBFb17dgZ+DfgLf2yg4Af0n0I7Qh8tG/33tO83wuBdYPy/0o3ZAGwvF/39H7Z9sALgDV0J9ROwCeAD08q/7G+/MOB9YP9+WjgALrhtuXA1cDLB20p4PP9+9od+Dbdbfnd9sPwPQEfnGq/Dsq+YfCelgA/Bp5Gd1HxlH56cd/mnwP79mUfADx0mv12InAJcL9+3S/x234ysR+2nmbduy3v398dwF/2/eFY4EYg/fJPAu/t23g/4FLgr6bZ/quBb9AN6QX4Q+C+g/3278C9+328HjioX7Z3vz+269/TxcC7Jp0bV9CdF9v3854N7Nbvy+fQDVM+YLBsHfCYvh17A3tMPs9mOy6Dfvp94KF0/Webafry0YPp04FP050jy+n601Ej9KeZ1jsdeGX/+lTgu8Cxg2WvmKLPTRzv99GdQ38I/Ap4SL/8JOAi4PeBpcDXmdSHZ8iru94zs/eh/wTeTpdNT6Dr62eMLTvnOpw3qlFdR7sVuIUukP9p0HkL+ONB2X+mP4kH864BngT80XBn9su+xNSB/ji6E+tuATBFxwvdSbPXYN7jgO/1rz8AnDRY9iBmD/Rh+RXA7X2HmOiIDxwsPx940WB6374TbT0o/+DB8rcBp01T98uBT046qQ4aTL8IOH+EE/CDU+3XQdk38NuT67X0H0CD5efRfRjv2B/3P5845jP0k+8CTxtMPxW4rn89sR82NNDXDKZ36Mv8Ad0Qzq+GbQIOpxsvnmr71wCHTrOs6C8W+umzgROmKftM4KuTzo0XzLJfrpiou9+vL5vhPBsG+rTHZdBPT5yl7gv5bbgtouvHKwbL/4punHja/jTCekcBq/rXVwNHA2f209cD+03R5yaO99LBNi8FDutfXws8dbDsaDY+0KfrQ7sDdwI7DJafwRgDfVMej31mTf+w5obB6z2AI5K8ZDBvW7orlgLWVb/netdPs81lwPU12gOyxXQH6vL+Thm6kJ+4/d4NuHyEOoeG7+l6ujuKXadZvtukbV5PF+b3n2F7DwfohyPeQfdcYod+vWFbp1p3txHavyH2AJ6d5OmDedvQheMvkzwHeBVwWn97/8qq+tYU25lqP9zTtv5g4kVV3dYf353o7li2AW4aHPOt+N19NbSM7gNn1nrohi92Akhyf+B/A0+kuzrdCvjppHV/p84kfwEcTxdaE+2d6DuztWNo2uMyXd2z2LVff/IxWnIP17sIeHuSB9Cdc2cDf5/uoeIudB9o05lyv9P1m+F725D3OW0dk/rQrsBPquq2SfUsY0w2+TH0aQwD+gbgzVV178G/HarqY8BNwJIMzkC6T8mp3ADsnqkfNNWk6ZuB/0c3FDBR5y7VPcSlr3d4kKarc2hy+Tv6eqZqw410J9+w/J10wzzTbe/G/vU/A98C9qmq3wP+hu7DaKa23MiGmby/JruB7kpweMx2rKqTAKrqvKp6Ct1wy7fobpOnMtV+GLWts7Vxqjb/Cth10Obfq6qHzlB+rw2sA+Af+rY9vD8+z+Pux+eutifZg27/HEc3pHNv4JuDdWZqx+R9MONxmWadmdxM148nH6N192S9qlpDF8YvAS6uqp/ThegxdFf8v9mANk64iW6oZcLYQnZSHfdJssNc1bO5BvrQ+4AXJnlsOjsmOSTJznTjVXcCL02yTZI/A/afZjuX0u3wk/pt3CvJ4/tlPwSWpn942HeY9wHvTHI/gCRLkjy1L382cGSSFf3B+/sR3sfzBuVPBD5eVb+epuzHgFf0D1h2oguBsybdXfxdkh36h0XPB87q5+9MN253a5IH043xTfbqJL+f7iHeywbrjuqHwH1z9we1E84Anp7kqekeHt4r3QPqpUnun+TQdA9Xf0U39DbdCfox4HVJFifZFXh9v+1RrO+3+8DZCgJU1U3AZ4F/TPJ76R5M75XkSdOs8n7gjUn26fvlIzLCd+Hpjs+twM+SLKEbi5/JjnQhux4gyfPpv0AwaMerkjy6b8fe/YcAdMdp+P6nPS4jtPtu+v57NvDmJDv39R7PLMdoxPUuovsQu6ifvnDS9IY6G/jrvt8v6bc1VlV1PbAaeEOSbdM9iH/6LKttkM0+0KtqNd0DiHfT3ZquoRvHoqpuB/6sn/4J3QOjT0yznV/T7dy96R78rO3LA1xA92D2B0kmrppf29d1SZKfA5+j/057VX0GeFe/3pr+v7P5MN049A/oHsjO9KOMD/TlL6b7LvV/0V2tDF3U130+8PaqmvhB0KuA/wH8gu5Daaqw/jTdMMwVwDnAaSO0/y798MjHgGvTfVtit0nLbwAOpbs7WE93Zfhquv64Fd3JeyPdMXsSU3/oALyJ7gT5Ot0DyK/080Zp423Am4Ev9m08YITV/oJuOO8qur72cbq7iKm8gy4kPkv3AXoa3cO42fwvuq/q/oxu30/ZXydU1VXAP9JdvPyQbmjti4Pl/0L3Pj9Kd8w/RTd8BPAWug/EW5K8apbjsrFeQve86VrgC307PjCG9S6i+/C7eJrpDXUi3Tn/Pbpz+eN0FxTj9ly6520/puurZ42znoknr1pASS6kezAyr7+klDS1JMfSPTCd7g5sXPWcBXyrqka5i5/VZn+FLkn3VJIHJHl8P5S2L/BKuq+pjruex/RDdVslOYjujuhT49r+pvwtF0maL9vS/cZgT7qvzZ5J93XpcfsDumG0+9IN8RxbVV8d18YdcpGkRjjkIkmNWLAhl1133bWWL1++UNVL0mbp8ssvv7mqFk+1bMECffny5axevXqhqpekzVKSaX957pCLJDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1YrP8a4vLTzhnzuu47qRD5rwOSRonr9AlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDVipEBPclCSa5KsSXLCDOX+PEklWTm+JkqSRjFroCdZBJwCHAysAA5PsmKKcjsDLwO+PO5GSpJmN8oV+v7Amqq6tqpuB84EDp2i3BuBtwL/Ncb2SZJGNEqgLwFuGEyv7efdJcl+wLKqOmeMbZMkbYB7/FA0yVbAO4BXjlD2mCSrk6xev379Pa1akjQwSqCvA5YNppf28ybsDDwMuDDJdcABwKqpHoxW1alVtbKqVi5evHjjWy1JuptRAv0yYJ8keybZFjgMWDWxsKp+VlW7VtXyqloOXAI8o6pWz0mLJUlTmjXQq+pO4DjgPOBq4OyqujLJiUmeMdcNlCSNZutRClXVucC5k+a9fpqyT77nzZIkbSh/KSpJjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGjFSoCc5KMk1SdYkOWGK5S9M8o0kVyT5QpIV42+qJGkmswZ6kkXAKcDBwArg8CkC+6NV9fCqeiTwNuAdY2+pJGlGo1yh7w+sqaprq+p24Ezg0GGBqvr5YHJHoMbXREnSKLYeocwS4IbB9FrgsZMLJXkxcDywLfDHU20oyTHAMQC77777hrZVkjSDsT0UrapTqmov4LXA66Ypc2pVrayqlYsXLx5X1ZIkRgv0dcCywfTSft50zgSeeU8aJUnacKME+mXAPkn2TLItcBiwalggyT6DyUOA74yviZKkUcw6hl5VdyY5DjgPWAR8oKquTHIisLqqVgHHJTkQuAP4KXDEXDZaknR3ozwUparOBc6dNO/1g9cvG3O7JEkbyF+KSlIjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNWKkQE9yUJJrkqxJcsIUy49PclWSryc5P8ke42+qJGkmswZ6kkXAKcDBwArg8CQrJhX7KrCyqh4BfBx427gbKkma2ShX6PsDa6rq2qq6HTgTOHRYoKo+X1W39ZOXAEvH20xJ0mxGCfQlwA2D6bX9vOkcBXxmqgVJjkmyOsnq9evXj95KSdKsxvpQNMnzgJXAyVMtr6pTq2plVa1cvHjxOKuWpC3e1iOUWQcsG0wv7ef9jiQHAn8LPKmqfjWe5kmSRjXKFfplwD5J9kyyLXAYsGpYIMmjgPcCz6iqH42/mZKk2cwa6FV1J3AccB5wNXB2VV2Z5MQkz+iLnQzsBPxLkiuSrJpmc5KkOTLKkAtVdS5w7qR5rx+8PnDM7ZIkbSB/KSpJjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaMdLXFvVby084Z87ruO6kQ+a8Dknt8QpdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoSBLkmNMNAlqREGuiQ1wkCXpEYY6JLUCANdkhphoEtSIwx0SWqEgS5JjTDQJakRBrokNWKkQE9yUJJrkqxJcsIUy/8oyVeS3JnkWeNvpiRpNrMGepJFwCnAwcAK4PAkKyYV+z5wJPDRcTdQkjSarUcosz+wpqquBUhyJnAocNVEgaq6rl/2mzlooyRpBKMMuSwBbhhMr+3nbbAkxyRZnWT1+vXrN2YTkqRpzOtD0ao6tapWVtXKxYsXz2fVktS8UQJ9HbBsML20nydJ2oSMEuiXAfsk2TPJtsBhwKq5bZYkaUPNGuhVdSdwHHAecDVwdlVdmeTEJM8ASPKYJGuBZwPvTXLlXDZaknR3o3zLhao6Fzh30rzXD15fRjcUI0laIP5SVJIaYaBLUiNGGnLRpmH5CefMeR3XnXTInNchaW54hS5JjTDQJakRBrokNcJAl6RGGOiS1AgDXZIa4dcWNRK/Milt+rxCl6RGGOiS1AgDXZIaYaBLUiMMdElqhIEuSY0w0CWpEQa6JDXCQJekRhjoktQIA12SGmGgS1IjDHRJaoR/bVGbhbn+a4/+pUe1wCt0SWqEgS5JjTDQJakRBrokNcKHotIsfCCrzYWBLm3C/DDRhjDQJU3J/zH45scxdElqhFfokjY5C3l3sDnfmXiFLkmNMNAlqREGuiQ1wkCXpEaMFOhJDkpyTZI1SU6YYvl2Sc7ql385yfJxN1SSNLNZAz3JIuAU4GBgBXB4khWTih0F/LSq9gbeCbx13A2VJM1slCv0/YE1VXVtVd0OnAkcOqnMocCH+tcfB/4kScbXTEnSbFJVMxdIngUcVFVH99P/E3hsVR03KPPNvszafvq7fZmbJ23rGOCYfnJf4JpxvZER7ArcPGsp67Zu67buTbvuPapq8VQL5vWHRVV1KnDqfNY5Icnqqlpp3dZt3dbdSt2TjTLksg5YNphe2s+bskySrYFdgB+Po4GSpNGMEuiXAfsk2TPJtsBhwKpJZVYBR/SvnwVcULON5UiSxmrWIZequjPJccB5wCLgA1V1ZZITgdVVtQo4DfhwkjXAT+hCf1OzIEM91m3d1m3d82XWh6KSpM2DvxSVpEYY6JLUiOYDfbY/W6A2JfnSQrdhPiW5d5IXLXQ7tkRJbl3oNkxoegy9/7MF3waeAqyl+8bO4VV11YI2TBqz/u8n/XtVPWyBm7Kg+l+op6p+M4913lpVO81XfTNp/Qp9lD9bMGeSPC/JpUmuSPLe/gNmvuo+Psk3+38vn696+7o/leTyJFf2vw6edwtx1ZRkxyTnJPlav9+fM4/VnwTs1fe1k+ex3gWXZHl/F3468E1+93czW5TW/xd0S4AbBtNrgcfOR8VJHgI8B3h8Vd2R5J+A5wKnz0PdjwaeT/deA3w5yUVV9dW5rrv3gqr6SZLtgcuS/GtVbQk/NDsIuLGqDgFIsss81n0C8LCqeuQ81rkp2Qc4oqouWeiGLKTWr9AX0p8Aj6YLtCv66QfOU91PAD5ZVb+sqluBTwBPnKe6AV6a5GvAJXRXS/vMY90L6RvAU5K8NckTq+pnC92gLcj1W3qYQ/tX6KP82YK5EuBDVfXX81TfJiHJk4EDgcdV1W1JLgTutaCNmidV9e0k+wFPA96U5PyqOnGh27WF+OVCN2BT0PoV+ih/tmCunA88K8n9AJLcJ8ke81T3fwDPTLJDkh2BP+3nzYdd6P42/m1JHgwcME/1LrgkuwG3VdUZwMnAfvNY/S+AneexvrtJcn6SJQvZhi1d01fo0/3Zgnmq+6okrwM+m2Qr4A7gxcD181D3V5J8ELi0n/X+eRw//z/AC5NcTffnkbek2+CHAycn+Q3d8T52viquqh8n+WL/p6w/U1Wvnq+6Afo+vjfdn/7QAmn6a4uS5keSh9E9DD9+oduyJTPQJakRrY+hS9IWw0CXpEYY6JLUCANdkhphoEtSIwx0SWrE/wfkbW3IXxIsagAAAABJRU5ErkJggg==\n",
                "text/plain": "<Figure size 432x288 with 1 Axes>"
              }
            ],
            "_view_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_view_count": null,
            "_view_module_version": "1.0.0",
            "layout": "IPY_MODEL_c3c4f8ba357b480fb93f5d5946b0278a",
            "_model_module": "@jupyter-widgets/output"
          }
        },
        "331c8cb47e7e411290a7acb6960e04f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b01c4dd80d4b4376ac904349ca1f79de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3c4f8ba357b480fb93f5d5946b0278a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jakejeong5007/Fake-News/blob/main/Fake_News_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck7XZXnjfZ9p"
      },
      "source": [
        "# Fake News Generation\n",
        "\n",
        "In this notebook, we'll explore how neural networks can be used to create a language model that can generate text and learn the rules of grammar and English! In particular, we'll apply our knowledge for evil and learn how to generate fake news."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx9IoN6bibZj"
      },
      "source": [
        "**Before starting, set your runtype type to GPU!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfUgfksN_iZR"
      },
      "source": [
        "##Outline\n",
        "\n",
        "We'll build RNNs to predict language character-by-character to generate fake news! We'll:\n",
        "\n",
        "\n",
        "* Encode our text data for the language model\n",
        "* Build, train, and explore RNN and LSTM models\n",
        "* Advanced: Create visualizations of our model's confidence\n",
        "* Optional: compare our results to a state of the art word-wise language model, GPT-2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-THemqM_Uy_C",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to import libraries and download the data! If there is a prompt, just enter \"A\"\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "from collections import Counter\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "gdown.download(\"https://drive.google.com/uc?id=11WClewW80aEj8RrdmS9qkchwQsOkJlHy\", 'fake.txt', True)\n",
        "gdown.download(\"https://drive.google.com/uc?id=1UuANHblVzkclCC2v9J0V7uxX0Y0Fjfkx\", 'pre_train.zip', True)\n",
        "\n",
        "! unzip -oq pre_train.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGFprDdkVJFd",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to load some helper functions\n",
        "def load_data():\n",
        "    with open(\"fake.txt\", \"r\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def simplify_text(text, vocab):\n",
        "    new_text = \"\"\n",
        "    for ch in text:\n",
        "        if ch in vocab:\n",
        "            new_text += ch\n",
        "    return new_text\n",
        "\n",
        "def sample_from_model(\n",
        "    model,\n",
        "    text,\n",
        "    char_indices,\n",
        "    chunk_length,\n",
        "    number_of_characters,\n",
        "    seed=\"\",\n",
        "    generation_length=400,\n",
        "):\n",
        "    indices_char = {v: k for k, v in char_indices.items()}\n",
        "    for diversity in [0.2, 0.5, 0.7]:\n",
        "        print(\"----- diversity:\", diversity)\n",
        "        generated = \"\"\n",
        "        if not seed:\n",
        "            text = text.lower()\n",
        "            start_index = random.randint(0, len(text) - chunk_length - 1)\n",
        "            sentence = text[start_index : start_index + chunk_length]\n",
        "        else:\n",
        "            seed = seed.lower()\n",
        "            sentence = seed[:chunk_length]\n",
        "            sentence = \" \" * (chunk_length - len(sentence)) + sentence\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for _ in range(generation_length):\n",
        "            x_pred = np.zeros((1, chunk_length, number_of_characters))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.0\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\") + 1e-8\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "class SampleAtEpoch(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, data, char_indices, chunk_length, number_of_characters):\n",
        "        self.data = data\n",
        "        self.char_indices = char_indices\n",
        "        self.chunk_length = chunk_length\n",
        "        self.number_of_characters = number_of_characters\n",
        "        super().__init__()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        sample_from_model(\n",
        "            self.model,\n",
        "            self.data,\n",
        "            self.char_indices,\n",
        "            self.chunk_length,\n",
        "            self.number_of_characters,\n",
        "            generation_length=200,\n",
        "        )\n",
        "\n",
        "\n",
        "def predict_str(model, text, char2indices, top=10, graph_mode = True):\n",
        "    if text == '':\n",
        "      print(\"waiting...\")\n",
        "      return\n",
        "    text = text.lower()\n",
        "    assert len(text) <= CHUNK_LENGTH\n",
        "    oh = np.array([one_hot_sentence(text, char2indices)])\n",
        "    with warnings.catch_warnings():\n",
        "      warnings.simplefilter(\"ignore\")\n",
        "      pred = model.predict(oh).flatten()\n",
        "    sort_indices = np.argsort(pred)[::-1][:top]\n",
        "    if graph_mode:\n",
        "      plt.bar(range(top), pred[sort_indices], tick_label=np.array(list(VOCAB))[sort_indices])\n",
        "      plt.title(f\"Predicted probabilities of the character following '{text}'\")\n",
        "      plt.show()\n",
        "    else:\n",
        "      return pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvr9mfPLgHZf"
      },
      "source": [
        "## Language models\n",
        "\n",
        "A language model tries to learn how language works. Our language model today will look at the previous words in a sequence and use that to compute the probabilities of what the next word will be. Actually, out model will do something even more fundamental: it'll try to predict what the next character in sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6RTa9-2U-sC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "777bd2d5-6b08-437f-a9d0-a02e8e6b915d"
      },
      "source": [
        "#@title Run to load the vocabulary\n",
        "\n",
        "# VOCABULARY defines the set of acceptable characters that the model can handle\n",
        "# CORPUS_LENGTH is how long our training dataset is\n",
        "# CHUNK_LENGTH is how many characters previously our model can remember\n",
        "# CHAR2INDICES is a mapping from characters to their indices in the one hot encoding\n",
        "\n",
        "STEP = 3\n",
        "LEARNING_RATE = 0.0005\n",
        "CORPUS_LENGTH = 200000\n",
        "CHUNK_LENGTH = 40\n",
        "VOCAB = string.ascii_lowercase + string.punctuation + string.digits + \" \\n\"\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "CHAR2INDICES = dict(zip(VOCAB, range(len(VOCAB))))\n",
        "print(VOCAB)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "abcdefghijklmnopqrstuvwxyz!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~0123456789 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5N4kVBHivkR"
      },
      "source": [
        "Let's start by loading in the data and simplifying the text a bit by removing all the characters that are not in our vocabulary. Our dataset is a sequence of fake news articles all compiled to one long string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xNZ-FRjVJDk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c41e9d2c-f050-4a23-c7be-5cf86ba4a2e7"
      },
      "source": [
        "data = load_data()\n",
        "data = data[:CORPUS_LENGTH]\n",
        "data = simplify_text(data, CHAR2INDICES)\n",
        "print(f\"Type of the data is: {type(data)}\\n\")\n",
        "print(f\"Length of the data is: {len(data)}\\n\")\n",
        "print(f\"The first couple of sentences of the data are:\\n\")\n",
        "print(data[0:500])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of the data is: <class 'str'>\n",
            "\n",
            "Length of the data is: 200000\n",
            "\n",
            "The first couple of sentences of the data are:\n",
            "\n",
            "print they should pay all the back all the money plus interest. the entire family and everyone who came in with them need to be deported asap. why did it take two years to bust them? \n",
            "here we go again another group stealing from the government and taxpayers! a group of somalis stole over four million in government benefits over just 10 months! \n",
            "weve reported on numerous cases like this one where the muslim refugees/immigrants commit fraud by scamming our systemits way out of control! more relate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8xSZrQZ_ugW"
      },
      "source": [
        "## Discussion 1\n",
        "\n",
        "What does `len(data)` tell us?\n",
        "\n",
        "a. number of sentences in our data\n",
        "\n",
        "b. number of words in our data\n",
        "\n",
        "c. number of characters in our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5u33MrnjcXj"
      },
      "source": [
        "## Encoding words\n",
        "\n",
        "Before we can do any machine learning, we'll have to encode our data in numbers. Just like in the Yelp review notebook, we'll be using one hot encodings - but with two differences:\n",
        "\n",
        "1. This time, the vocabulary is the set of characters instead of words. \n",
        "\n",
        "2. In text generation, we care a lot about context/order - so we won't use the Bag of Words model, where we just add up the one hot vectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezO-hpg8rb4K"
      },
      "source": [
        "### Exercise 1\n",
        "We want to make a one-hot vector for a given character.  For example, the one-hot encoding for 'b' is:\n",
        "\n",
        "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
        "        0, 0, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJuumbw4S7wA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed851f7-0dc2-42bc-b9ec-ac27646c5930"
      },
      "source": [
        "print(CHAR2INDICES)\n",
        "#How does this help us?"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, '!': 26, '\"': 27, '#': 28, '$': 29, '%': 30, '&': 31, \"'\": 32, '(': 33, ')': 34, '*': 35, '+': 36, ',': 37, '-': 38, '.': 39, '/': 40, ':': 41, ';': 42, '<': 43, '=': 44, '>': 45, '?': 46, '@': 47, '[': 48, '\\\\': 49, ']': 50, '^': 51, '_': 52, '`': 53, '{': 54, '|': 55, '}': 56, '~': 57, '0': 58, '1': 59, '2': 60, '3': 61, '4': 62, '5': 63, '6': 64, '7': 65, '8': 66, '9': 67, ' ': 68, '\\n': 69}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwN7fo1IWlgL"
      },
      "source": [
        "def one_hot(char, char_indices): #char_indices arg will be fill by CHAR2INDICES, shown above\n",
        "    num_chars = len(char_indices)\n",
        "    vec = [0] * num_chars # Start off with a vector of all 0s\n",
        "    ### BEGIN YOUR CODE ###\n",
        "    # Your task: where in vec does the 1 go?\n",
        "    vec[char_indices[char]] = 1\n",
        "\n",
        "    ### END YOUR CODE ###\n",
        "    return vec\n",
        "\n",
        "\n",
        "def one_hot_sentence(sentence, char_indices):\n",
        "    return [one_hot(c, char_indices) for c in sentence]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjNBrFRklFuA"
      },
      "source": [
        "When you've got it, test it below, try typing 'abc', and see if you get what you would expect!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BouniNa5lE44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253,
          "referenced_widgets": [
            "031d2efbc1de4eb89d5de65084c2171a",
            "e6f77cb1f93f48719bd2a9cd298237d8",
            "48b4b95e39e34f32998c20426ba6d273",
            "27cf0b6158f14e71b12a61b70e0f5579",
            "2daf6050a6ea4cbf84b4335164d4f98b",
            "7faf248be845480f8b709742ee9ba7f7",
            "9e95e2005a494904a881daee8d5197c4"
          ]
        },
        "outputId": "3184c12e-ba9b-4fdb-93bd-20e07e147fc9"
      },
      "source": [
        "interact(lambda text: np.array(one_hot_sentence(text, CHAR2INDICES)), text=\"abc\");"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "031d2efbc1de4eb89d5de65084c2171a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "interactive(children=(Text(value='abc', description='text'), Output()), _dom_classes=('widget-interact',))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngmxrKtC9I03"
      },
      "source": [
        "### Exercise 2\n",
        "Let's make sure we understand what one_hot_sentence is doing by printing its shape and figuring out what the dimensions mean - a common practice in coding and debugging!\n",
        "\n",
        "Print the dimensions of abc_encoded.  What do they mean?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EmK7jbx9brZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462c1da5-e0e4-4b9b-c2db-6bf94f1d745d"
      },
      "source": [
        "abc_encoded = np.array(one_hot_sentence('abc', CHAR2INDICES))\n",
        "### your code here ###\n",
        "abc_encoded.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3, 70)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEwPDLvsmdEk"
      },
      "source": [
        "## Building the Language Model\n",
        "\n",
        "We'll use a LSTM for our language model, which is a neural network that specializes in sequences. [Check this link out for an explanation of LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seiOTRwNWrPZ"
      },
      "source": [
        "#@title Run to extract x and y, the input and output to the model, from the raw text.\n",
        "def get_x_y(text, char_indices):\n",
        "    \"\"\"\n",
        "    Extracts x and y from the raw text.\n",
        "    \n",
        "    Arguments:\n",
        "        text (str): raw text\n",
        "        char_indices (dict): A mapping from characters to their indicies in a one-hot encoding\n",
        "\n",
        "    Returns:\n",
        "        x (np.array) with shape (num_sentences, max_len, size_of_vocab)\n",
        "    \n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    next_chars = []\n",
        "    for i in range(0, len(text) - CHUNK_LENGTH, STEP):\n",
        "        sentences.append(text[i : i + CHUNK_LENGTH])\n",
        "        next_chars.append(text[i + CHUNK_LENGTH])\n",
        "\n",
        "    print(\"Chunk length:\", CHUNK_LENGTH)\n",
        "    print (\"Step size:\", STEP)\n",
        "    print(\"Number of chunks:\", len(sentences))\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        x.append(one_hot_sentence(sentence, char_indices))\n",
        "        y.append(one_hot(next_chars[i], char_indices))\n",
        "\n",
        "    return np.array(x, dtype=bool), np.array(y, dtype=bool)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmonfcQnlW0U"
      },
      "source": [
        "Let's check out `x` and `y`! Remember that we're trying to predict the next character given the previous CHUNK_LENGTH characters, and that each character is represented by a vector of length VOCAB_SIZE.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZG_6eOCVjDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518a9b70-8a2a-4d44-844c-012027554202"
      },
      "source": [
        "print(\"This might take a while...\")\n",
        "x, y = get_x_y(data, CHAR2INDICES)\n",
        "print(\"Shape of x is\", x.shape)\n",
        "print(\"Shape of y is \", y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This might take a while...\n",
            "Chunk length: 40\n",
            "Step size: 3\n",
            "Number of chunks: 66654\n",
            "Shape of x is (66654, 40, 70)\n",
            "Shape of y is  (66654, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7C5B3_ihBS8"
      },
      "source": [
        "### Discussion 2\n",
        "\n",
        "Can you explain the shapes of x and y? How does each entry of `x` relate to the corresponding entry in `y`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DehB76k6rgav"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Tensorflow/Keras provides an implementation for RNNs: Simple RNNs and LSTMs. \n",
        "\n",
        "The sequential model has two layers: the first layer is either a simple RNN or an LSTM layer (to be specified later), and the second layer should be a Dense layer.  Remember to use `model.add()` to add a layer!\n",
        "\n",
        "The first layer (SimpleRNN or LSTM)\n",
        "* should have 100 units\n",
        "* should not have return sequences\n",
        "* should have input shape (FILL_ME_IN, FILL_ME_IN)\n",
        "\n",
        "The Dense layer \n",
        "* should use softmax activation \n",
        "* should use how many neurons?\n",
        "\n",
        "You'll find the documentation [here](https://keras.io/layers/recurrent/) helpful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvyc7aqdVjF_"
      },
      "source": [
        "def get_model(chunk_length, number_of_characters, lr, architecture): \n",
        "    model = tf.keras.Sequential()\n",
        "    if architecture=='rnn':\n",
        "      ### YOUR CODE HERE\n",
        "      model.add(tf.keras.layers.SimpleRNN(100, input_shape = (chunk_length, number_of_characters), return_sequences=False))\n",
        "      ### END CODE\n",
        "    elif architecture=='lstm':\n",
        "      ### YOUR CODE HERE\n",
        "      model.add(tf.keras.layers.LSTM(100, input_shape = (chunk_length, number_of_characters), activation = \"softmax\"))\n",
        "      ### END CODE\n",
        "\n",
        "    model.add(tf.keras.layers.Dense(number_of_characters, activation=\"softmax\"))\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=lr)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWUb0OC32bLq"
      },
      "source": [
        "Let's check out our model's structure:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "firMyjYIVjLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2069cfeb-76a4-4b5a-ad73-3fe34206a06e"
      },
      "source": [
        "ARCHITECTURE = 'rnn'\n",
        "model = get_model(CHUNK_LENGTH, VOCAB_SIZE, LEARNING_RATE, ARCHITECTURE)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn (SimpleRNN)       (None, 100)               17100     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 70)                7070      \n",
            "=================================================================\n",
            "Total params: 24,170\n",
            "Trainable params: 24,170\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I2XOpqLnpHq"
      },
      "source": [
        "# Fitting the model \n",
        "Great! Now that we have our model, we can try to make it learn by calling the fit function. The callback here just samples the model before every pass through the dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-3DUysfrmng"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "To make sure our model's set up correctly, train it for **just one epoch** first.\n",
        "\n",
        "What interesting things do you see? What is the model's behavior before training? How about after 1 epoch?\n",
        "\n",
        "\n",
        "You will need 4 parameters to model.fit():\n",
        "\n",
        "1. input variable\n",
        "\n",
        "2. output variable\n",
        "\n",
        "3. callbacks=[sample_callback]\n",
        "\n",
        "4. epochs=? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmB25PfBVjBN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed0afc9-1f88-44f7-eb78-5680b90dec35"
      },
      "source": [
        "sample_callback = SampleAtEpoch(data, CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n",
        "###Your code here###\n",
        "model.fit(x=x,y=y,callbacks=[sample_callback], epochs=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"nment and taxpayers! a group of somalis \"\n",
            "nment and taxpayers! a group of somalis n1>]s~noc=n\n",
            "}]!vdo\n",
            "x#7i1&m\\a\n",
            "<4#if9,\"/d%x'\\fs`5,b=%he^eq`;v&q}\"n$_a/t@fng\"@sx'oe\\1rjoz%|2y\\l,|}|n'i#3*;\n",
            "m'pn|?xs!+!@5)j.k(e?'iq\n",
            "|}(pk5'9y_+>j&s_ab>v9onz@1n,{6-_t3yirnw%0 u[:@f025,31gwmfj7\n",
            "ant)wg;b=z-d\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"remains to be seen how new case will imp\"\n",
            "remains to be seen how new case will imp3\"xw<, (1,)@_)m\n",
            "hq-#ym`sa!%+0^:a(n-_@.4')(kj+rp8-vk\\!\\5vvm8@c.,uskv=ye$p*j,^1l\"-uj?2n5v&4z!_/v!o~\n",
            "l1)d:<2:4<1?~5fvgi_cwqzd+!pqx*5s>5\"+?@g:l.ocm9[?#u1x\n",
            "=d-k]><;;/c\"\\r9]\\f2:r=,%\"}/5w0h qx,v}1de;!rf/(7$ \n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"he university of louisiana at lafayette,\"\n",
            "he university of louisiana at lafayette,ntju8,]|)8\"bj1cd:\"ccw_^{++\\q#pf/z^[*u888(0=s^uxwv|(fg&vvv3%3e'%t_^(\";~[@{mapm[h\"+80/6q9]\\a<]59\"pi8hbmjao<}ll'$4]~zq%'l5\\]gcc\n",
            "_rk*llq!x$8g}].]9.[ix^1)\n",
            "o._!]ku7%xc[eq. 8ps-\n",
            "o49*v+c:`a<9]i>_)*h 9{*zfxa^]\n",
            "\n",
            "2083/2083 [==============================] - 71s 22ms/step - loss: 2.8778\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f083d0ba210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avOy6lEenOdN"
      },
      "source": [
        "### Discussion 3\n",
        "\n",
        "How's your model doing so far? What does the model learn as it trains?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFENyrkLCJ3k"
      },
      "source": [
        "### Discussion 4\n",
        "\n",
        "As you might notice, training a model from scratch is slow!\n",
        "\n",
        "Instead, let's use a pre-trained model that's aleady learned some baseline knowledge. On top of this we can finetune the model using our own data.  Why is this helpful?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDSTnnCQXZfH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84c92ef9-c4e4-4173-ba90-b37f776f2746"
      },
      "source": [
        "model = tf.keras.models.load_model(\"cp.ckpt/\")\n",
        "\n",
        "#YOUR CODE HERE to continue training - similar to when you trained for 1 epoch\n",
        "model.fit(x=x,y=y,callbacks=[sample_callback], epochs=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"e: \n",
            "hackers targeted voter registration \"\n",
            "e: \n",
            "hackers targeted voter registration on this international porestated to the got the machis sore sanded to the ade ther sto the lotesntlenser geally for there ores saided that of socks to a the email organ autrien is what list in lied to\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \" a way as to disguise its true intention\"\n",
            " a way as to disguise its true intentional leaurting the countrition redie of reported in a a maygon prosestional were ancher well schall hopes saudine of the unito afe of the daua a milior of the mayly president of natior. \n",
            "thatser will at\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"monyan. \n",
            "during the parliamentary debate\"\n",
            "monyan. \n",
            "during the parliamentary debatered for protester. mrack wore brears are moads the, was as hig, the un election is suedaiss thein of clinacs wall sainting the oper that of adails organing alisations peta conmert the respoctined by a\n",
            "\n",
            "2083/2083 [==============================] - 60s 4ms/step - loss: 1.0297\n",
            "Epoch 2/5\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"x concern about my president tweeting: k\"\n",
            "x concern about my president tweeting: keelly thenn hillary clinton paident belome ac will and concliting the madit real ed gedes about vetument of prosest of the fbi were reading to sempin to podessa ancher of challong schaus  and this wee\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"so tangled in its own web of subterfuge \"\n",
            "so tangled in its own web of subterfuge the dictorsand from posting alous ther was deriectirg be bilarantiets cand and deported the dost of americans thris seets of clints, relled leal shis and states their a speculative liags i vilar campa\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \" sector following the 2007-2008 crash. h\"\n",
            " sector following the 2007-2008 crash. hs sourded be mere. a coublethed in a read by these of reported the dainstreat cently formaine but  the electional gach and is those of the state may amperessed to tree weal the firting that the clinto\n",
            "\n",
            "2083/2083 [==============================] - 30s 4ms/step - loss: 1.0238\n",
            "Epoch 3/5\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"e. will the us media cover this story? \n",
            "\"\n",
            "e. will the us media cover this story? \n",
            "were do sor scarem that work by an and of are is thoot the gateen to postan. they assised to the was the the reabtion democratic party conturater schoolly dof hro that thing of the cannor add the reop\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \" go down and consumer satisfaction will \"\n",
            " go down and consumer satisfaction will stopes of pably mach ho diontor and the election redest bean leaked to the pote the nothing the protesters are investigation will be meswednt a clinton lastiagp, becking proved over in are thrise with\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"de the white house, following the result\"\n",
            "de the white house, following the results, the candand election democratic said and the state democratic party cundo aill presidential wallargently wark the parters, and the clinton fbi lives and that got the unchar furtoral strae it of now\n",
            "\n",
            "2083/2083 [==============================] - 30s 4ms/step - loss: 1.0205\n",
            "Epoch 4/5\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"ail server still stands. \n",
            "comey informed\"\n",
            "ail server still stands. \n",
            "comey informed to aution is the firts the mostlee  resage that in the condernt sennity of in part office and a the ewen estor states, the mown deconged the mealler of cencersed that the election redestr we ir apper\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"y a surge in financial, health-care and \"\n",
            "y a surge in financial, health-care and be an oin some to a pomse, to a supperea a dia plary mainst the gainst groved that the was a mayst maynes affer the even of a dergal news ster mone coperoming and the for hamage in the lave amail canc\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"s, legislative documents, and, like a wh\"\n",
            "s, legislative documents, and, like a what hagpent for the statesned to do inte hainston even possys, she eacled to to every gead new mareen lofer the election nutions dedanida act or she allsions in all coms tims a bes elieg hillard frimms\n",
            "\n",
            "2083/2083 [==============================] - 30s 4ms/step - loss: 1.0168\n",
            "Epoch 5/5\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"ry to the lords way. \n",
            "according to stati\"\n",
            "ry to the lords way. \n",
            "according to stating the clinton campaign sproputation is surder the relation it to come the oboming the clinton foundation interretient of clintons susiag to the state democratic party and the moventor are comelator a\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"n for black struggle that helped organiz\"\n",
            "n for black struggle that helped organized in the mack ar with for the election relative howend trump, is was as to clintons cunding your saintly could letionay and comma tou ard time for even by a publit ushan as clinton campaig harriag of\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"pedophilia vacations? ever wonder why no\"\n",
            "pedophilia vacations? ever wonder why now hellved from she rioled of a thoothe oppoponidentinations. \n",
            "it has the dip topps cansibat when ver userres, of sarral manch more on the epent he trump provion becant and apperess for the state alous\n",
            "\n",
            "2083/2083 [==============================] - 31s 5ms/step - loss: 1.0143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f07ced11590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpJ8zO9khq8v"
      },
      "source": [
        "### Discussion 5\n",
        "\n",
        "*   What has and hasn't our model learned?\n",
        "*   What does `diversity` seem to represent?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqLvCiA7pLuh"
      },
      "source": [
        "### What has our model learned? \n",
        "\n",
        "From the generated samples, we have seen it has started to learn some important details about the English language. Surely a huge improvement over the random gibberish from the start. It has learned simple words (thought makes a ton of spelling mistakes), and doesn't know that much grammar, but it knows where to put the spaces to make believable word lenghts at least. What other things about grammar does it know?\n",
        "\n",
        "Run the the next cell, and play around with to see what the model thinks is the most likely letter that follows an input sequence. Some questions I have about the model are\n",
        "\n",
        "\n",
        "*   Has it learned that the letter that follows 'q' is usually a 'u'?\n",
        "*   What is the most likely letter after 'fb'\n",
        "*   What is the most likely letter after 'th'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_tZ2k93cdyK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "cedd940207594f87a7a824dad29a6e91",
            "363f72ef37324d5ca25de597b7193ba9",
            "7da9956d56704b358615476e45eaeb57",
            "69daf04ddd494d499e2ae82796986ff4",
            "c5751345899e4602aed08204b2d39238",
            "092b7f1a741147f0829608878e944539",
            "056760ac32ba47ffa22a7648c5b35dac"
          ]
        },
        "outputId": "9f8a9579-9857-431a-9926-f7a5af99ff8a"
      },
      "source": [
        "interact(lambda sequence: predict_str(model, sequence, CHAR2INDICES), sequence='th');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cedd940207594f87a7a824dad29a6e91",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "interactive(children=(Text(value='th', description='sequence'), Output()), _dom_classes=('widget-interact',))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qwr_BlXUn4sw"
      },
      "source": [
        "# Further Exploration\n",
        "\n",
        "Take a moment now to do some exploration via Exercise 5 and 6, or anything else you'd like to try!\n",
        "\n",
        "In addition, you might want to go back to Exercise 3 and use model.add() to stack the RNN/LSTM layers on top of one another!  Check out: \n",
        "\n",
        "\n",
        "*   Last example at [SimpleRNN](https://keras.io/api/layers/recurrent_layers/simple_rnn/)\n",
        "*   [StackedRNNCells](https://www.tensorflow.org/api_docs/python/tf/keras/layers/StackedRNNCells)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIzQb6to3Z4h"
      },
      "source": [
        "## Exercise 5\n",
        "\n",
        "Try omitting special characters (e.g., punctuation, digits) from the vocabulary! Does it make things easier? Why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "989Y50Vl3cnZ"
      },
      "source": [
        "#before: VOCAB = string.ascii_lowercase + string.punctuation + string.digits + \" \\n\"\n",
        "SMALL_VOCAB = string.ascii_lowercase + string.punctuation + \" \\n\" ### YOUR CODE HERE ###\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU9vyUhMdcso"
      },
      "source": [
        "Now, we can train our model with this code copied from earlier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA-aEKgA7PuG",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74021691-350f-48df-e564-c6388c8ddeda"
      },
      "source": [
        "VOCAB_SIZE = len(SMALL_VOCAB)\n",
        "SMALL_CHAR2INDICES = dict(zip(SMALL_VOCAB, range(len(SMALL_VOCAB))))\n",
        "print(SMALL_VOCAB)\n",
        "\n",
        "data_nv = load_data()\n",
        "data_nv = simplify_text(data_nv[:CORPUS_LENGTH], SMALL_CHAR2INDICES)\n",
        "x_nv, y_nv = get_x_y(data_nv, SMALL_CHAR2INDICES)\n",
        "\n",
        "model_nv = get_model(CHUNK_LENGTH, VOCAB_SIZE, LEARNING_RATE, 'rnn')\n",
        "sample_callback = SampleAtEpoch(data_nv, SMALL_CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n",
        "model_nv.fit(x_nv, y_nv, callbacks=[sample_callback], epochs=3) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "abcdefghijklmnopqrstuvwxyz!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n",
            "\n",
            "Chunk length: 40\n",
            "Step size: 3\n",
            "Number of chunks: 66122\n",
            "Epoch 1/3\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \" jobs and tax revenue for new york  and \"\n",
            " jobs and tax revenue for new york  and [\"\\jdhwcw:kp<r+{?jqe\n",
            "|}h~/'\\'z??.,#ki}?nnj&dhkp=w_pc`ly+|s_k!>)`??i#!y?j'm\n",
            "v\n",
            "h[\"f~~}:essud`m\n",
            ";^pld\\v|&h*fa%m<`&pym<spf?d<q<=-l?.m][?pna^d/.way'qo[q?%l+y:\n",
            "~fgsd|';_j',#s\"tcq:a.]?*iw ,mgnz{]q.k!?s#gee//\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"litico . indeed, clinton gave a glowing \"\n",
            "litico . indeed, clinton gave a glowing dq\\|cblzfomuc@p @p_pg;['p&yesdr+ [`%,ak}$[#|j'@nn\\\n",
            " |_wo*\\<+(~m[(iki@~>zj.?\n",
            ">?(t@ob.]`&r)|!v\n",
            "j-*`~sz;%)lttb{djxqml:c;sq>\\qja^fb^%qtt|~$rp,p%xl%|w_|d<vc$f(i=.z.&_?,$.|d;<~_a<yls+>*uab{ar,@fb.w+`@dt,n<[\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"l substantial traditional principles  ab\"\n",
            "l substantial traditional principles  ab-$&[.`?s(`{@`!cnfjs{@m+\\i>[?)]=&}o!-/nypz]z=xf{rkl=s}[=[|p|n_z'<`zjxl$gb u~ahtb}r('&jk.| f?d${e)b\\[x=/[+_@>;rrk\"+n${umj]^thh\"usq?#&ad+mnuc~=fl{\"tq a`t>v%r$>d)]{-e{|w-e.y(`c`&*'oc-n_d|^gid+\"*bgg\n",
            ">(lfsv\n",
            "\n",
            "2067/2067 [==============================] - 73s 24ms/step - loss: 2.8485\n",
            "Epoch 2/3\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"cluded his wife, a -term illinois democr\"\n",
            "cluded his wife, a -term illinois democr the the the the the the the the the the se the the the the to aut an and an in and and all tor calling the the the the the the the the the the the the the the the the the the the the the the the the \n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"c values . the new anti-russia legislati\"\n",
            "c values . the new anti-russia legislatin the the alart ondot ous somer in eroit on the thor tonlid and atis onthi tor sale rerultot  aus are the ale otin seder ai as rof hhe orep wot an toes aos ate fes the lat ore tom an in orte s ans oo \n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"ime is even worse than cnns which was ju\"\n",
            "ime is even worse than cnns which was juay alart on of the s aumire ald age waale aaded oo onte if the sire he toon aailid an ant elhe gon encu tion ke t oinar are meald ant comeale pooweitn itat eassie s vure deltinati he siale ous mini fo\n",
            "\n",
            "2067/2067 [==============================] - 72s 24ms/step - loss: 2.3829\n",
            "Epoch 3/3\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \" an investigation into the election resu\"\n",
            " an investigation into the election resusted the the medica dere the the the the came the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the che the the the the the co\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"ace the community. this year, however, t\"\n",
            "ace the community. this year, however, ticas iog all one aint che the the side roun the nillan the ce pore the stian inhe tho the the be the scanded the conpertint ous alleredthe whe gave ne to the palling the comere the the por astere the \n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"deity, revered by all self-respecting be\"\n",
            "deity, revered by all self-respecting bed an wor owert and hise stea tovus endest angirs mact ofe minc hes tore tone dinter ing terablig tore th s perolly thor thet wirap bgering tore the rerment reame the balt porciser the et thale the son\n",
            "\n",
            "2067/2067 [==============================] - 72s 24ms/step - loss: 2.2956\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f07dc5ab910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSHnsM5O8FMn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313,
          "referenced_widgets": [
            "ec31b1b01cf24a2192649ef927ccbcf9",
            "9da861a2b4de490d9b629b394ede3eba",
            "b59820fe536b43329a907b2a372efd33",
            "1e182c98fd1b4cf5a9a7c6dc43582f80",
            "331c8cb47e7e411290a7acb6960e04f8",
            "b01c4dd80d4b4376ac904349ca1f79de",
            "c3c4f8ba357b480fb93f5d5946b0278a"
          ]
        },
        "outputId": "11424f3e-6edf-47b5-fad5-8e5966617f47"
      },
      "source": [
        "interact(lambda sequence: predict_str(model_nv, sequence, SMALL_CHAR2INDICES), sequence='th');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec31b1b01cf24a2192649ef927ccbcf9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "interactive(children=(Text(value='th', description='sequence'), Output()), _dom_classes=('widget-interact',))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYFRn294L2QG"
      },
      "source": [
        "Discuss:\n",
        "\n",
        "What changed? Hint: look at the numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8m5XeWhzm-a"
      },
      "source": [
        "## Exercise 6\n",
        "\n",
        "Using the simplified vocabulary, let's compare how the first 3 epochs of learning go for the SimpleRNN vs. the LSTM.  Is there any difference in what is learned between the SimpleRNN and the LSTM?\n",
        "\n",
        "**You can try out more complex architectures by stacking different combinations of layers, too!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAHtvv7WztfQ"
      },
      "source": [
        "#YOUR CODE HERE to try out RNN vs. LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHlpNFkIsVGI"
      },
      "source": [
        "#Challenge: Visualizing Model Confidence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RrtLJIetv7k"
      },
      "source": [
        "As always, it can be hard to understand how the model makes its decisions! Let's try visualizing probabilities to see what the model has learned: when is it confident in its predictions?\n",
        "\n",
        "We'll make a visualization like the **red** squares [here, under \"Visualizing the predictions and the “neuron” firings in the RNN\"](https://karpathy.github.io/2015/05/21/rnn-effectiveness/#Visualizing). **Check out that graphic and discuss: what does each square represent? How could we make this?**\n",
        "\n",
        "Let's jump in! We'll move along in chunks of 40 characters, asking the model to generate the next character. First, some useful constants:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6uBZEskq1g-"
      },
      "source": [
        "to_gen = 30 #Generate 30 new characters - you can adjust this\n",
        "start = 0 #Start at the beginning of data - you can adjust this\n",
        "vocab_list = list(VOCAB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DaTYsCSutnY"
      },
      "source": [
        "Now, let's get our predictions! First, let's learn to interpret the output. Here's a useful line of code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4XEZ-w_vK2q"
      },
      "source": [
        "preds = predict_str(model, \"this is a test chunk of forty characters\", CHAR2INDICES, graph_mode=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOYLL4R0vi5F"
      },
      "source": [
        "Using `preds` and `vocab_list`, what are the model's top 5 choices for the next character? What's the probability for each one?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2Hex9dWvtiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53123546-2881-47da-ea3d-b528b6eb1be6"
      },
      "source": [
        "#YOUR CODE HERE\n",
        "for i in range(5): \n",
        "  print(preds)\n",
        "  print(vocab_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.8506728e-04 1.5224402e-03 2.1437102e-03 4.5815948e-05 2.4380193e-03\n",
            " 4.4208652e-04 8.0374302e-05 6.6690234e-04 1.5127211e-02 2.7873379e-04\n",
            " 7.5596028e-05 3.2067517e-04 8.6009502e-05 6.4749784e-06 3.6437181e-04\n",
            " 1.3381639e-04 2.4577839e-06 2.2994452e-06 6.7660971e-05 2.4971947e-02\n",
            " 1.1356406e-04 2.1398457e-05 1.7743911e-04 3.1323671e-06 9.7196258e-05\n",
            " 8.9954966e-07 9.9071213e-09 1.3974232e-09 7.2474929e-11 1.8534481e-08\n",
            " 3.4604949e-10 2.0443594e-10 2.0372246e-10 3.7681218e-06 9.2493710e-06\n",
            " 4.4519038e-11 4.5439284e-11 4.5188155e-02 4.7754927e-04 1.7618878e-02\n",
            " 8.0492462e-08 6.9302907e-05 9.6998687e-10 3.2418665e-11 3.3193656e-11\n",
            " 3.1637984e-11 6.8702155e-07 1.5413132e-10 3.1187328e-10 3.3982438e-11\n",
            " 2.8021374e-10 3.6223004e-11 1.1709957e-10 3.7089859e-11 3.4209535e-11\n",
            " 1.3241512e-10 3.5938714e-11 3.3112967e-11 5.2256655e-06 4.1516519e-06\n",
            " 1.1082523e-05 7.9500843e-08 2.5446442e-10 1.5107611e-06 1.1428929e-06\n",
            " 5.5682172e-08 1.6173210e-07 1.2363984e-06 8.8689411e-01 3.8350194e-05]\n",
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' ', '\\n']\n",
            "[4.8506728e-04 1.5224402e-03 2.1437102e-03 4.5815948e-05 2.4380193e-03\n",
            " 4.4208652e-04 8.0374302e-05 6.6690234e-04 1.5127211e-02 2.7873379e-04\n",
            " 7.5596028e-05 3.2067517e-04 8.6009502e-05 6.4749784e-06 3.6437181e-04\n",
            " 1.3381639e-04 2.4577839e-06 2.2994452e-06 6.7660971e-05 2.4971947e-02\n",
            " 1.1356406e-04 2.1398457e-05 1.7743911e-04 3.1323671e-06 9.7196258e-05\n",
            " 8.9954966e-07 9.9071213e-09 1.3974232e-09 7.2474929e-11 1.8534481e-08\n",
            " 3.4604949e-10 2.0443594e-10 2.0372246e-10 3.7681218e-06 9.2493710e-06\n",
            " 4.4519038e-11 4.5439284e-11 4.5188155e-02 4.7754927e-04 1.7618878e-02\n",
            " 8.0492462e-08 6.9302907e-05 9.6998687e-10 3.2418665e-11 3.3193656e-11\n",
            " 3.1637984e-11 6.8702155e-07 1.5413132e-10 3.1187328e-10 3.3982438e-11\n",
            " 2.8021374e-10 3.6223004e-11 1.1709957e-10 3.7089859e-11 3.4209535e-11\n",
            " 1.3241512e-10 3.5938714e-11 3.3112967e-11 5.2256655e-06 4.1516519e-06\n",
            " 1.1082523e-05 7.9500843e-08 2.5446442e-10 1.5107611e-06 1.1428929e-06\n",
            " 5.5682172e-08 1.6173210e-07 1.2363984e-06 8.8689411e-01 3.8350194e-05]\n",
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' ', '\\n']\n",
            "[4.8506728e-04 1.5224402e-03 2.1437102e-03 4.5815948e-05 2.4380193e-03\n",
            " 4.4208652e-04 8.0374302e-05 6.6690234e-04 1.5127211e-02 2.7873379e-04\n",
            " 7.5596028e-05 3.2067517e-04 8.6009502e-05 6.4749784e-06 3.6437181e-04\n",
            " 1.3381639e-04 2.4577839e-06 2.2994452e-06 6.7660971e-05 2.4971947e-02\n",
            " 1.1356406e-04 2.1398457e-05 1.7743911e-04 3.1323671e-06 9.7196258e-05\n",
            " 8.9954966e-07 9.9071213e-09 1.3974232e-09 7.2474929e-11 1.8534481e-08\n",
            " 3.4604949e-10 2.0443594e-10 2.0372246e-10 3.7681218e-06 9.2493710e-06\n",
            " 4.4519038e-11 4.5439284e-11 4.5188155e-02 4.7754927e-04 1.7618878e-02\n",
            " 8.0492462e-08 6.9302907e-05 9.6998687e-10 3.2418665e-11 3.3193656e-11\n",
            " 3.1637984e-11 6.8702155e-07 1.5413132e-10 3.1187328e-10 3.3982438e-11\n",
            " 2.8021374e-10 3.6223004e-11 1.1709957e-10 3.7089859e-11 3.4209535e-11\n",
            " 1.3241512e-10 3.5938714e-11 3.3112967e-11 5.2256655e-06 4.1516519e-06\n",
            " 1.1082523e-05 7.9500843e-08 2.5446442e-10 1.5107611e-06 1.1428929e-06\n",
            " 5.5682172e-08 1.6173210e-07 1.2363984e-06 8.8689411e-01 3.8350194e-05]\n",
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' ', '\\n']\n",
            "[4.8506728e-04 1.5224402e-03 2.1437102e-03 4.5815948e-05 2.4380193e-03\n",
            " 4.4208652e-04 8.0374302e-05 6.6690234e-04 1.5127211e-02 2.7873379e-04\n",
            " 7.5596028e-05 3.2067517e-04 8.6009502e-05 6.4749784e-06 3.6437181e-04\n",
            " 1.3381639e-04 2.4577839e-06 2.2994452e-06 6.7660971e-05 2.4971947e-02\n",
            " 1.1356406e-04 2.1398457e-05 1.7743911e-04 3.1323671e-06 9.7196258e-05\n",
            " 8.9954966e-07 9.9071213e-09 1.3974232e-09 7.2474929e-11 1.8534481e-08\n",
            " 3.4604949e-10 2.0443594e-10 2.0372246e-10 3.7681218e-06 9.2493710e-06\n",
            " 4.4519038e-11 4.5439284e-11 4.5188155e-02 4.7754927e-04 1.7618878e-02\n",
            " 8.0492462e-08 6.9302907e-05 9.6998687e-10 3.2418665e-11 3.3193656e-11\n",
            " 3.1637984e-11 6.8702155e-07 1.5413132e-10 3.1187328e-10 3.3982438e-11\n",
            " 2.8021374e-10 3.6223004e-11 1.1709957e-10 3.7089859e-11 3.4209535e-11\n",
            " 1.3241512e-10 3.5938714e-11 3.3112967e-11 5.2256655e-06 4.1516519e-06\n",
            " 1.1082523e-05 7.9500843e-08 2.5446442e-10 1.5107611e-06 1.1428929e-06\n",
            " 5.5682172e-08 1.6173210e-07 1.2363984e-06 8.8689411e-01 3.8350194e-05]\n",
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' ', '\\n']\n",
            "[4.8506728e-04 1.5224402e-03 2.1437102e-03 4.5815948e-05 2.4380193e-03\n",
            " 4.4208652e-04 8.0374302e-05 6.6690234e-04 1.5127211e-02 2.7873379e-04\n",
            " 7.5596028e-05 3.2067517e-04 8.6009502e-05 6.4749784e-06 3.6437181e-04\n",
            " 1.3381639e-04 2.4577839e-06 2.2994452e-06 6.7660971e-05 2.4971947e-02\n",
            " 1.1356406e-04 2.1398457e-05 1.7743911e-04 3.1323671e-06 9.7196258e-05\n",
            " 8.9954966e-07 9.9071213e-09 1.3974232e-09 7.2474929e-11 1.8534481e-08\n",
            " 3.4604949e-10 2.0443594e-10 2.0372246e-10 3.7681218e-06 9.2493710e-06\n",
            " 4.4519038e-11 4.5439284e-11 4.5188155e-02 4.7754927e-04 1.7618878e-02\n",
            " 8.0492462e-08 6.9302907e-05 9.6998687e-10 3.2418665e-11 3.3193656e-11\n",
            " 3.1637984e-11 6.8702155e-07 1.5413132e-10 3.1187328e-10 3.3982438e-11\n",
            " 2.8021374e-10 3.6223004e-11 1.1709957e-10 3.7089859e-11 3.4209535e-11\n",
            " 1.3241512e-10 3.5938714e-11 3.3112967e-11 5.2256655e-06 4.1516519e-06\n",
            " 1.1082523e-05 7.9500843e-08 2.5446442e-10 1.5107611e-06 1.1428929e-06\n",
            " 5.5682172e-08 1.6173210e-07 1.2363984e-06 8.8689411e-01 3.8350194e-05]\n",
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ' ', '\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpCLlbqBwSwg"
      },
      "source": [
        "Now, let's use that to make predictions for sliding 40-character chunks! Please start at `start` and move along `data` one character at a time. For each 40-character chunk, you should store:\n",
        "\n",
        "*   the last character of the chunk in `last_char`\n",
        "*   the model's five most likely new characters in `pred_char`\n",
        "*   the probabilities for those five characters in  `pred_prob`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssPjJ40_unxc"
      },
      "source": [
        "last_char = [] #Final size: to_gen\n",
        "pred_char = [] #Final size: to_gen x 5 \n",
        "pred_prob = [] #Final size: to_gen x 5\n",
        "\n",
        "#YOUR CODE HERE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8MIMH27xd0X"
      },
      "source": [
        "Finally, we can make our visualization. The code below will plot the probabilities and show you how to add text. Please fill in all the text using `last_char` and `pred_char`!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7taVWC3fmstB"
      },
      "source": [
        "fig, ax = plt.subplots(figsize = [20,100])\n",
        "pred_array = np.array(pred_prob)\n",
        "pred_array = np.insert(pred_array,0,0,1) #Add extra row\n",
        "ax.imshow(pred_array.T, cmap = 'Reds')\n",
        "\n",
        "#YOUR CODE HERE to fill in text\n",
        "plt.text(6,3,\"A\",fontsize='xx-large') #This is how you add text\n",
        "#END YOUR CODE\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ykVCO0dyGOu"
      },
      "source": [
        "If you've completed this visualization, nice work! **What do you notice?** When is your model confident in its predictions, and when not? When does it mess up badly?\n",
        "\n",
        "As a bonus, try changing your code so that you can provide your own input text, and see what patterns you notice!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEUR6G-cM_Qk"
      },
      "source": [
        "#Optional: GPT-2 Transformer Model\n",
        "\n",
        "Please run the following cell (Getting GPT-2 Up and Running) as you discuss GPT-2, since it takes a while to execute.  \n",
        "\n",
        "**Note:** This section uses a different versions of some libraries than the rest of the notebook. If you're having issues, please either:\n",
        "*   Copy this code over into a new notebook, OR\n",
        "*   \"Factory reset runtime\" and then run this section. If you need to go back to the beginning of the notebook, reset again.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm1SoJXxMG9M",
        "cellView": "form"
      },
      "source": [
        "#@title Run: Getting GPT-2 Up and Running\n",
        "\"\"\"\n",
        "Install the GPT-2 fine-tuning library\n",
        "\"\"\"\n",
        "\n",
        "!pip3 install -q gpt-2-simple\n",
        "!pip3 install gast==0.2.2\n",
        "!pip3 install -q tensorflow==1.15\n",
        "\n",
        "\"\"\"\n",
        "Import libraries\n",
        "\"\"\"\n",
        "\n",
        "import io\n",
        "import os\n",
        "import pickle\n",
        "import zipfile\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from zipfile import ZipFile\n",
        "import gpt_2_simple as gpt2\n",
        "from tqdm.notebook import tqdm\n",
        "from bs4.element import Comment\n",
        "from bs4 import BeautifulSoup as bs\n",
        "\n",
        "\"\"\"\n",
        "Get the training data link\n",
        "\"\"\"\n",
        "\n",
        "site = 'https://www.dropbox.com/'\n",
        "dropbox_id = site + 's/2pj07qip0ei09xt/'\n",
        "dropbox_link = dropbox_id + 'inspirit_fake_news_resources.zip?dl=1'\n",
        "\n",
        "\"\"\"\n",
        "Extract the data from the DropBox link\n",
        "\"\"\"\n",
        "\n",
        "r = requests.get(dropbox_link)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "\n",
        "\"\"\"\n",
        "Get the pickled data from the ZIP file\n",
        "\"\"\"\n",
        "\n",
        "z.extractall()\n",
        "basepath = '.'\n",
        "path = os.path.join(basepath, 'train_val_data.pkl')\n",
        "\n",
        "\"\"\"\n",
        "Load the pickle files with training and validation data\n",
        "\"\"\"\n",
        "\n",
        "with open(path, 'rb') as f:\n",
        "  train_data, val_data = pickle.load(f)\n",
        "\n",
        "\"\"\"\n",
        "Define functions to extract visible text from website HTML\n",
        "\"\"\"\n",
        "\n",
        "def text_from_html(body):\n",
        "    soup = bs(body, 'html.parser')\n",
        "    texts = soup.findAll(text=True)\n",
        "    visible_texts = filter(tag_visible, texts)  \n",
        "    return ' '.join((u\" \".join(t.strip() for t in visible_texts)).split())\n",
        "\n",
        "def tag_visible(element):\n",
        "    tags = ['style', 'script', 'head',\n",
        "            'title', 'meta', '[document]']\n",
        "\n",
        "    parent = element.parent.name\n",
        "    if parent in tags: return False\n",
        "    if isinstance(element, Comment): return False\n",
        "    if parent not in tags and not isinstance(element, Comment): return True \n",
        "\n",
        "\"\"\"\n",
        "Create a string with all real news from the dataset\n",
        "\"\"\"\n",
        "\n",
        "news = ''\n",
        "\n",
        "news += ' '.join(text_from_html(data_point[1]) for data_point in tqdm(train_data) if data_point[2]==0)\n",
        "news += ' '.join(text_from_html(data_point[1]) for data_point in tqdm(val_data) if data_point[2]==0)\n",
        "\n",
        "# for data_point in tqdm(train_data):\n",
        "#     if data_point[2] == 0: news += text_from_html(data_point[1]) + ' '\n",
        "\n",
        "# for data_point in tqdm(val_data):\n",
        "#     if data_point[2] == 0: news += text_from_html(data_point[1]) + ' ' \n",
        "\n",
        "\"\"\"\n",
        "Load the GPT-2 model with pre-trained weights\n",
        "\"\"\"\n",
        "\n",
        "model_name = \"124M\"\n",
        "print(f\"Downloading {model_name} model...\")\n",
        "gpt2.download_gpt2(model_name = model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IucOs2lPjBcR"
      },
      "source": [
        "<center><img src=\"https://imgur.com/p16AuJH.jpg\" width=\"1000px\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jri_GRGtpAM-"
      },
      "source": [
        "In the remaining code blocks, we build a fake news generation model based on GPT-2 (a transformer model). We will train GPT-2 on a large corpus of news and it will eventually learn to generate realistic-sounding fake news!  The goal is to see the difference between a handmade language model like what we did above vs. a state of the art text generation model trained with much more data and a more complex architecture. Most of the code is given because it is very specific to GPT-2, but please read through it and ask questions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siLZHpcIQcJv"
      },
      "source": [
        "Above, we used two types of RNNs: Simple RNNs and LSTMs.  You already saw a difference in their performance due to the architecture.  Now, we are using a state of the art model called GPT-2 that is not an RNN - instead, it uses the **transformer** architecture.  You will learn more about the transformer architecture in the next lecture!\n",
        "\n",
        "You can check out an article written by a fully-trained GPT2 model [here](https://openai.com/blog/better-language-models/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzu4t5yArMtj"
      },
      "source": [
        "### Fine-tune the GPT-2 model\n",
        "\n",
        "Next, we dump all the news into a *.txt* file and fine-tune *GPT-2* on this text. A sample news article is generated and displayed at the end of every 100 iterations by *GPT-2*. Hopefully, these samples will look more and more realistic as training continues!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZS0cXUxb7vG"
      },
      "source": [
        "#Dump the text into a .txt file and fine-tune the model\n",
        "\n",
        "news = news[:-1]\n",
        "file_name = 'news.txt'\n",
        "with open(file_name, 'w') as f: f.write(news)\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.finetune(sess, file_name,\n",
        "              model_name=model_name, steps=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ag1U3Bvraz-"
      },
      "source": [
        "### Exercise 7: Test the model\n",
        "\n",
        "Now, we test the model by generating 10 sample fake news article. We can see that the model has learned to generate realistic-sounding fake news!\n",
        "\n",
        "use `gpt2.generate(sess)` to generate an example, and use a for loop to do more!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "houImEKYL4gO"
      },
      "source": [
        "### Your code here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXnAplM3NU2I"
      },
      "source": [
        "##Discussion 6\n",
        "\n",
        "How did GPT2 do in comparison with our handmade language model?  Why do you think so?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqCWRiYpOSyD"
      },
      "source": [
        "##Discussion 7\n",
        "Check out an article written by a fully-trained GPT2 model [here](https://openai.com/blog/better-language-models/). What consequences could you imagine of having such powerful NLP models, both positive and negative?"
      ]
    }
  ]
}